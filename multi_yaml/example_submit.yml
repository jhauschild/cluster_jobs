#!/bin/bash 

job_config:
    class: JobConfig  # JobConfig = run locally; SlurmJob = submit to slurm; SGEJob = submit to SGE
    jobname: MyJob
    task:  # specify what your job actually does
        type: PythonFunctionCall
        module: simulation
        function: run_simulation
        # extra_imports: 
        #     - my_other_py_module

    # script_template: slurm.sh  # select from cluster_templates/
    # # adjust the following lines to tell the cluster the resource requirements
    # requirements_slurm:
    #     time: '0:30:00'
    #     memory:
    #     nodes: 1
    # requirements_sge:
    #     l: 'h_cpu=0:30:00,h_vmem=5G'  # CPU (wall) time and RAM (per core) requirements
    #     q: 'cond-mat'  # queue to submit to
    # options:
    #     cores_per_task: 2

    change_parameters:
        expansion: product  # product or zip
        recursive_keys:
            - a
            - sub_params.c
        # value_lists: 
        #     - [ 128, 256, 512]
        #     - [0.5, 1.]
        #     # you can either specify all the value_lists for all parameters here, 
        #     # or have the lists distributed over the rest of the yaml file(s).
        # format_strs:  # this allows formatting of output_filename
        #     - 'a_{0:04d}'
        #     - 'c_{0:.2f}'

        output_filename:
            # key: output_filename
            prefix: 'output'
            suffix: '.pkl'

a: [100, 500, 1000]
b: !py_eval |
    2*np.pi
# cluster_jobs.py supports an extra !py_eval tag for evaluating a python code snippet.
# Like pickle, this is a security nightmare, so don't use untrusted configs from the internet!

sub_params:
    c: !py_eval |
       np.arange(0.5, 1.5, 0.5)
    d: 2
